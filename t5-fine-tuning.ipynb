{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":""},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/google-t5/t5-base\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google-t5/t5-base)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"google-t5/t5-base\")","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Remote Inference via Inference Providers \nEnsure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\nThe following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you. \nFor more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index).","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"import os\nos.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    provider=\"auto\",\n    api_key=os.environ[\"HF_TOKEN\"],\n)\n\nresult = client.translation(\n    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –í–æ–ª—å—Ñ–≥–∞–Ω–≥ –∏ —è –∂–∏–≤—É –≤ –ë–µ—Ä–ª–∏–Ω–µ\",\n    model=\"google-t5/t5-base\",\n)","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null}]}