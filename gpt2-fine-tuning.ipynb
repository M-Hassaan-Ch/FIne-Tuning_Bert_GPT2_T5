{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":""},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/openai-community/gpt2\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/openai-community/gpt2)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")","metadata":{"colab_type":"code"},"outputs":[],"execution_count":null}]}