{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install -q -U transformers datasets peft accelerate tokenizers\n\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntrain_csv = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\"\noutput_dir = \"/kaggle/working/t5-lora-summarization\"\nmodel_name = \"t5-small\"\n\n\nsample_fraction = 0.025\n\n# Hyperparameters\nmax_input_length = 512\nmax_target_length = 128\ntrain_batch_size = 8\ngradient_accumulation_steps = 2\nnum_train_epochs = 3\nlearning_rate = 3e-4\nfp16 = torch.cuda.is_available()\n\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"Loading dataset...\")\ndf = pd.read_csv(train_csv)\n\n# Sample 10% of data\ndf_sample = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\nprint(f\"Using {len(df_sample)} samples ({sample_fraction*100}% of data)\")\n\n# I am creating my dataset\nds_train = Dataset.from_pandas(df_sample[[\"article\", \"highlights\"]])\n\nprint(\"Loading model and tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n\nprefix = \"summarize: \"\n\ndef preprocess_function(examples):\n    inputs = [prefix + str(x) for x in examples[\"article\"]]\n    targets = [str(x) for x in examples[\"highlights\"]]\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=max_input_length, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    labels = tokenizer(\n        text_target=targets, \n        max_length=max_target_length, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    # Replace padding token id with -100 for loss calculation\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"Tokenizing dataset...\")\ntokenized_train = ds_train.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=ds_train.column_names\n)\n\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer, \n    model=model, \n    label_pad_token_id=-100\n)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    learning_rate=learning_rate,\n    num_train_epochs=1,\n    logging_steps=350,\n    save_steps=500,\n    save_total_limit=2,\n    fp16=fp16,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"Starting training...\")\ntrainer.train()\n\nprint(\"Saving model...\")\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")\n\ndef summarize_text(text, max_length=128, num_beams=4):\n    input_text = prefix + text\n    inputs = tokenizer(\n        input_text, \n        return_tensors=\"pt\", \n        truncation=True, \n        max_length=max_input_length\n    ).to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=max_length, \n        num_beams=num_beams, \n        early_stopping=True\n    )\n    \n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return summary\n\n# Quick test\nprint(\"-\"*80)\nfor i in range(2):\n    item = ds_train[i]\n    print(item[\"article\"][:300] + \"...\")\n    print(f\"\\nREFERENCE: {item['highlights']}\")\n    print(f\"\\nSUMMARY: {summarize_text(item['article'])}\")\n    print(\"-\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T22:21:23.960348Z","iopub.execute_input":"2025-11-16T22:21:23.960699Z","iopub.status.idle":"2025-11-16T22:24:59.374713Z","shell.execute_reply.started":"2025-11-16T22:21:23.960666Z","shell.execute_reply":"2025-11-16T22:24:59.373888Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Loading dataset...\nUsing 7178 samples (2.5% of data)\nLoading model and tokenizer...\ntrainable params: 147,456 || all params: 60,654,080 || trainable%: 0.2431\nTokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c64ccfc3e64bd08823c9583e5024ba"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3366672132.py:124: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 02:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving model...\nModel saved to /kaggle/working/t5-lora-summarization\n--------------------------------------------------------------------------------\nBy . Mia De Graaf . Britons flocked to beaches across the southern coast yesterday as millions look set to bask in glorious sunshine today. Temperatures soared to 17C in Brighton and Dorset, with people starting their long weekend in deck chairs by the sea. Figures from Asda suggest the unexpected s...\n\nREFERENCE: People enjoyed temperatures of 17C at Brighton beach in West Sussex and Weymouth in Dorset .\nAsda claims it will sell a million sausages over long weekend despite night temperatures dropping to minus 1C .\nBut the good weather has not been enjoyed by all as the north west and Scotland have seen heavy rain .\n\nSUMMARY: Temperatures soared to 17C yesterday in Brighton and Dorset. Forecasters predict dry and sunny weather across southern England, southern Wales and the south Midlands.\n--------------------------------------------------------------------------------\nA couple who weighed a combined 32st were shamed into slimming by their own family - during Christmas dinner. Margaret Gibson, 37, and her husband, James, 41, from Biddulph, Staffs, started piling on the pounds after the birth of their two children just over a decade ago. But after taunts during the...\n\nREFERENCE: Couple started piling on pounds after the birth of two children .\nMargaret Gibson weighed 12st 5lb and husband James weighed 20st .\nJames Gibson's barred from simple op as he 'would die', warned doctor .\n\nSUMMARY: Margaret Gibson, 37, and her husband James, 41, from Biddulph, Staffs, started piling on the pounds after the birth of their children just over a decade ago. By last Christmas Margaret tipped the scales at 12st 5lb and James weighed an unhealthy 20st. But James's waist measurement has shrunk from 44in to 34in.\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ---------------------------------\n# 1.  create a ZIP of the whole folder\n# ---------------------------------\nimport shutil, os\nzip_path = \"/kaggle/working/t5_final_model.zip\"\nshutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', \"/kaggle/working/t5-lora-summarization\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T22:16:42.131181Z","iopub.execute_input":"2025-11-16T22:16:42.131522Z","iopub.status.idle":"2025-11-16T22:16:42.582337Z","shell.execute_reply.started":"2025-11-16T22:16:42.131498Z","shell.execute_reply":"2025-11-16T22:16:42.581640Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/t5_final_model.zip'"},"metadata":{}}],"execution_count":2}]}