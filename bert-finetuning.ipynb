{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13696662,"sourceType":"datasetVersion","datasetId":8712241}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\n\ncsv_path = \"/kaggle/input/sentiment-bert/sentiment-analysis-extended-v2.csv\"\n\n# Read single-column CSV\ndf = pd.read_csv(csv_path, header=None, names=[\"raw\"])\n\nprint(df.head())\n\n# Function to extract text + sentiment\ndef extract_text_and_label(row):\n    line = str(row)\n\n    # Regex:\n    #   1) quoted text\n    #   2) Positive or Negative\n    match = re.match(r'\\\"(.+?)\\\",\\s*(Positive|Negative)', line, flags=re.IGNORECASE)\n\n    if match:\n        text = match.group(1).strip()\n        label = match.group(2).strip().lower()\n        return pd.Series([text, label])\n    else:\n        # return None so we can drop problematic rows later\n        return pd.Series([None, None])\n\ndf[['text', 'label']] = df['raw'].apply(extract_text_and_label)\n\n# Drop rows that failed extraction\ndf = df.dropna(subset=['text', 'label'])\n\n# Map labels to 0/1\nlabel2id = {'negative': 0, 'positive': 1}\ndf['label_id'] = df['label'].map(label2id)\n\nprint(df.head())\n\n# Split 80/10/10\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)\nval_df, test_df   = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label_id'], random_state=42)\n\nprint(\"Splits:\", len(train_df), len(val_df), len(test_df))\n\nprint(\"10 rows of train_df:\")\n\nprint(train_df[['text']].sample(10, random_state=42))","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:56:06.225755Z","iopub.execute_input":"2025-11-11T22:56:06.225924Z","iopub.status.idle":"2025-11-11T22:56:11.048872Z","shell.execute_reply.started":"2025-11-11T22:56:06.225907Z","shell.execute_reply":"2025-11-11T22:56:11.048012Z"}},"outputs":[{"name":"stdout","text":"                                                 raw\n0  Text, Sentiment, Source, Date/Time, User ID, L...\n1  \"I love this product!\", Positive, Twitter, 202...\n2  \"The service was terrible.\", Negative, Yelp Re...\n3  \"This movie is amazing!\", Positive, IMDb, 2023...\n4  \"I'm so disappointed with their customer suppo...\n                                                 raw  \\\n1  \"I love this product!\", Positive, Twitter, 202...   \n2  \"The service was terrible.\", Negative, Yelp Re...   \n3  \"This movie is amazing!\", Positive, IMDb, 2023...   \n4  \"I'm so disappointed with their customer suppo...   \n5  \"Just had the best meal of my life!\", Positive...   \n\n                                               text     label  label_id  \n1                              I love this product!  positive         1  \n2                         The service was terrible.  negative         0  \n3                            This movie is amazing!  positive         1  \n4  I'm so disappointed with their customer support.  negative         0  \n5                Just had the best meal of my life!  positive         1  \nSplits: 404 50 51\n10 rows of train_df:\n                                                  text\n44   The product I ordered arrived damaged. Very di...\n230           Incredible product experience, loved it!\n354             This camera deserves no praise at all.\n383  This game is terrible and a complete waste of ...\n284  Super happy with this update! It's worth every...\n404  Super happy with this camera! It's worth every...\n72   The product I received was of poor quality. It...\n413                     I absolutely love this device!\n209                           I really hate this game.\n6               The quality of this product is subpar.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:57:24.859628Z","iopub.execute_input":"2025-11-11T22:57:24.860450Z","iopub.status.idle":"2025-11-11T22:57:34.317335Z","shell.execute_reply.started":"2025-11-11T22:57:24.860426Z","shell.execute_reply":"2025-11-11T22:57:34.316445Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport evaluate\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\n\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\n# Tokenization function for datasets\nmax_length = 128\n\ndef tokenize_fn(batch):\n    return tokenizer(\n        batch[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n    )\n\n# Build hf datasets from pandas\ntrain_ds = Dataset.from_pandas(train_df[['text','label_id']].reset_index(drop=True))\nval_ds   = Dataset.from_pandas(val_df[['text','label_id']].reset_index(drop=True))\ntest_ds  = Dataset.from_pandas(test_df[['text','label_id']].reset_index(drop=True))\n\n# rename label column to 'label' (expected by transformers)\ntrain_ds = train_ds.rename_column(\"label_id\", \"label\")\nval_ds   = val_ds.rename_column(\"label_id\", \"label\")\ntest_ds  = test_ds.rename_column(\"label_id\", \"label\")\n\ndataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n\n# map tokenization (batched)\ndataset = dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length),\n                      batched=True)\n\n# set format to PyTorch tensors\ndataset.set_format(type=\"torch\", columns=['input_ids','attention_mask','label','token_type_ids'])\n\nnum_labels = 2\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n# Simplified metrics - only accuracy\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    acc = accuracy.compute(predictions=preds, references=labels)\n    return {\"accuracy\": acc[\"accuracy\"]}\n\n# Training arguments with UPDATED parameter names\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-sentiment\",\n    eval_strategy=\"epoch\",           # CHANGED: evaluation_strategy -> eval_strategy\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    save_total_limit=2,\n    fp16=True,\n    logging_dir=\"./logs\",\n    logging_strategy=\"epoch\",        \n    report_to=\"none\",               \n    disable_tqdm=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Starting training...\")\ntrain_output = trainer.train()\n\n# Evaluate on test set\nprint(\"\\nEvaluating on test set...\")\ntest_metrics = trainer.evaluate(dataset[\"test\"])\nprint(f\"Test Loss: {test_metrics['eval_loss']:.4f}\")\nprint(f\"Test Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n\n# Save final model and tokenizer\ntrainer.save_model(\"/kaggle/working/bert-sentiment-final\")\ntokenizer.save_pretrained(\"/kaggle/working/bert-sentiment-final\")\n\nprint(\"\\nModel saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:57:39.926833Z","iopub.execute_input":"2025-11-11T22:57:39.927759Z","iopub.status.idle":"2025-11-11T22:58:13.572430Z","shell.execute_reply.started":"2025-11-11T22:57:39.927714Z","shell.execute_reply":"2025-11-11T22:58:13.571718Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a82fb9f4fe9f411580323da8b0b36330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64a393ed88d444c7b33ca0c9fa9f251d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ce518832ab4a328d250e2578999b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e67fbbdb25c49df9adaa953602c5edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/404 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"433da3a85e85462b938cb4b61de1e97e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f39eb16c20445678f7e113bad1b32c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/51 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a1fd5b260a43bdba1a2730f3953890"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0771554ccc304e9fba3c1b270c870ef9"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b325d15b1c41dcb8d9256a13719106"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3965110252.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [39/39 00:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.553500</td>\n      <td>0.341788</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.291300</td>\n      <td>0.229894</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.213300</td>\n      <td>0.188164</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test Loss: 0.3327\nTest Accuracy: 1.0000\n\nModel saved successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import pipeline\nclf = pipeline(\"text-classification\", model=\"/kaggle/working/bert-sentiment-final\", tokenizer=tokenizer, return_all_scores=False)\n\nexamples = [\n    \"I love this product!\",\n    \"The service was terrible.\",\n    \"I'm so disappointed with their customer support.\"\n]\n\nprint(clf(examples))\n# Output will include label name (e.g., 'LABEL_1') and score. You can map label ids back to label names:\nid2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:58:36.727426Z","iopub.execute_input":"2025-11-11T22:58:36.728024Z","iopub.status.idle":"2025-11-11T22:58:37.054810Z","shell.execute_reply.started":"2025-11-11T22:58:36.727998Z","shell.execute_reply":"2025-11-11T22:58:37.053758Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[{'label': 'LABEL_1', 'score': 0.86412513256073}, {'label': 'LABEL_0', 'score': 0.6569504141807556}, {'label': 'LABEL_0', 'score': 0.6091249585151672}]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ---------------------------------\n# 1.  create a ZIP of the whole folder\n# ---------------------------------\nimport shutil, os\nzip_path = \"/kaggle/working/bert-sentiment-final.zip\"\nshutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', \"/kaggle/working/bert-sentiment-final\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:59:12.952944Z","iopub.execute_input":"2025-11-11T22:59:12.953751Z","iopub.status.idle":"2025-11-11T22:59:34.994231Z","shell.execute_reply.started":"2025-11-11T22:59:12.953724Z","shell.execute_reply":"2025-11-11T22:59:34.993501Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/bert-sentiment-final.zip'"},"metadata":{}}],"execution_count":6}]}