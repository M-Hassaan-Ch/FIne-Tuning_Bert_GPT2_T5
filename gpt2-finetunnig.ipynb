{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13697679,"sourceType":"datasetVersion","datasetId":8713007}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings, torch, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nwarnings.filterwarnings(\"ignore\", message=\"fan_in_fan_out is set to False.*\")\n\n# ------------------------------------------------------------------\n# 1.  Load TSV\n# ------------------------------------------------------------------\ndef load_tsv_data(file_path: str) -> pd.DataFrame:\n    df = pd.read_csv(file_path, sep='\\t', header=None)\n    df = df.iloc[1:, :2]          # drop header row + keep 2 cols\n    df.columns = ['pseudo_code', 'code']\n    return df.dropna()\n\nfile_path = \"/kaggle/input/code-dataset/spoc-train.tsv\"   # <— change if needed\ndf = load_tsv_data(file_path)\n\ndef create_prompt(row):\n    return f\"\"\"Translate the following pseudo-code to working code:\nPseudo-code: {row['pseudo_code']}\nCode: {row['code']}\n\"\"\"\n\ndf['text'] = df.apply(create_prompt, axis=1)\ndf = df.sample(n=50_000, random_state=42)          # <-- NEW\n\n\n# ------------------------------------------------------------------\n# 2.  80 / 10 / 10 split\n# ------------------------------------------------------------------\ntrain_df, temp_df = train_test_split(df, test_size=0.20, random_state=42, shuffle=True)\nval_df, test_df   = train_test_split(temp_df, test_size=0.50, random_state=42, shuffle=True)\n\nprint(f\"Train: {len(train_df)}  |  Val: {len(val_df)}  |  Test: {len(test_df)}\")\n\n# ------------------------------------------------------------------\n# 3.  Tokeniser & model\n# ------------------------------------------------------------------\nmodel_name = \"openai-community/gpt2\"\ntokenizer  = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# ------------------------------------------------------------------\n# 4.  Tokenise datasets\n# ------------------------------------------------------------------\nblock_size = 256\n\ndef tokenise(examples):\n    tok = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=block_size,\n        return_special_tokens_mask=False,\n    )\n    tok[\"labels\"] = tok[\"input_ids\"].copy()\n    return tok\n\nraw_ds = DatasetDict({\n    \"train\":     Dataset.from_pandas(train_df, preserve_index=False),\n    \"validation\": Dataset.from_pandas(val_df, preserve_index=False),\n    \"test\":      Dataset.from_pandas(test_df,  preserve_index=False),\n})\n\ntokenized_ds = raw_ds.map(\n    tokenise,\n    batched=True,\n    remove_columns=raw_ds[\"train\"].column_names,\n    desc=\"Tokenising\",\n)\n\n# ------------------------------------------------------------------\n# 5.  Trainer\n# ------------------------------------------------------------------\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n    pad_to_multiple_of=8,\n)\n\nargs = TrainingArguments(\n    output_dir=\"./gpt2-pseudo-code-translator\",\n    eval_strategy=\"steps\",\n    eval_steps=200,       \n    save_strategy=\"steps\",\n    save_steps=500,\n    learning_rate=1e-4,\n     per_device_train_batch_size=16,   # ↑ from 2\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=1,    # ← remove acc\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_steps=50,\n    save_total_limit=2,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    dataloader_pin_memory=False,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nprint(\"Starting training...\")\ntrainer.train()\ntrainer.save_model(\"/kaggle/working/gpt2-pseudo-code-translator-final\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T21:58:10.215646Z","iopub.execute_input":"2025-11-11T21:58:10.216204Z","iopub.status.idle":"2025-11-11T22:27:10.634289Z","shell.execute_reply.started":"2025-11-11T21:58:10.216178Z","shell.execute_reply":"2025-11-11T22:27:10.633697Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/130016667.py:19: DtypeWarning: Columns (2,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path, sep='\\t', header=None)\n","output_type":"stream"},{"name":"stdout","text":"Train: 40000  |  Val: 5000  |  Test: 5000\ntrainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8607\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenising:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0660c5cfb14e21a4a791bf2a706672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenising:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5174d001a984e79a333ce558b26cb2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenising:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1628c8feb9474a9eae59930e35e33aca"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/130016667.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 28:46, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.515200</td>\n      <td>0.885828</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.432900</td>\n      <td>0.796761</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.398300</td>\n      <td>0.758671</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.383200</td>\n      <td>0.734461</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.369200</td>\n      <td>0.721284</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.376000</td>\n      <td>0.712118</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.380200</td>\n      <td>0.705470</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.370700</td>\n      <td>0.698094</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.369800</td>\n      <td>0.693501</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.374500</td>\n      <td>0.690403</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.361600</td>\n      <td>0.687701</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.345400</td>\n      <td>0.686283</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# ------------------------------------------------------------------\n# 1.  Load base GPT-2 + tokenizer\n# ------------------------------------------------------------------\nbase_model = \"openai-community/gpt2\"\ntokenizer  = AutoTokenizer.from_pretrained(base_model)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nbase = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# ------------------------------------------------------------------\n# 2.  Load the LoRA checkpoint you trained\n# ------------------------------------------------------------------\nlora_path = \"/kaggle/working/gpt2-pseudo-code-translator-final\"  # <- your save dir\nmodel = PeftModel.from_pretrained(base, lora_path)\nmodel.eval()\n\n# ------------------------------------------------------------------\n# 3.  Generation helper (unchanged except model->model)\n# ------------------------------------------------------------------\ndef generate_code(pseudo_code, model=model, tokenizer=tokenizer, max_length=256):\n    prompt = f\"Translate the following pseudo-code to working code:\\n\\nPseudo-code: {pseudo_code}\\n\\nCode:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text.split(\"Code:\")[-1].strip() if \"Code:\" in generated_text else generated_text\n\n# ------------------------------------------------------------------\n# 4.  Quick test\n# ------------------------------------------------------------------\nif __name__ == \"__main__\":\n    test_pseudo = \"print hello world\"\n    print(\"Generated code:\", generate_code(test_pseudo))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:47:23.241000Z","iopub.execute_input":"2025-11-11T22:47:23.241551Z","iopub.status.idle":"2025-11-11T22:47:28.631301Z","shell.execute_reply.started":"2025-11-11T22:47:23.241525Z","shell.execute_reply":"2025-11-11T22:47:28.630636Z"}},"outputs":[{"name":"stdout","text":"Generated code: cout << \"Hello!\" << endl;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n;\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ---------------------------------\n# 1.  create a ZIP of the whole folder\n# ---------------------------------\nimport shutil, os\nzip_path = \"/kaggle/working/gpt2-pseudo-code-translator-final.zip\"\nshutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', \"/kaggle/working/gpt2-pseudo-code-translator-final\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T22:52:13.979172Z","iopub.execute_input":"2025-11-11T22:52:13.979700Z","iopub.status.idle":"2025-11-11T22:52:14.712859Z","shell.execute_reply.started":"2025-11-11T22:52:13.979676Z","shell.execute_reply":"2025-11-11T22:52:14.712179Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/gpt2-pseudo-code-translator-final.zip'"},"metadata":{}}],"execution_count":30}]}